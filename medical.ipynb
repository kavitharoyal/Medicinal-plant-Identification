{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10671156,"sourceType":"datasetVersion","datasetId":6609462},{"sourceId":10672089,"sourceType":"datasetVersion","datasetId":6610149}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **NEW APPROACH WITH IDENTIFY* ***","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport matplotlib.pyplot as plt\nimport pickle\nimport os\n\n# Load plant details dataset\ndf = pd.read_excel(\"/kaggle/input/sci-data/sci.xlsx\")  # Update with your file path\n\n# Set dataset path\ndata_dir = \"/kaggle/input/med-images/images\"  # Update with your dataset path\nimg_size = (224, 224)\nbatch_size = 32\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T14:18:47.468105Z","iopub.execute_input":"2025-02-09T14:18:47.468416Z","iopub.status.idle":"2025-02-09T14:18:47.515616Z","shell.execute_reply.started":"2025-02-09T14:18:47.468393Z","shell.execute_reply":"2025-02-09T14:18:47.514703Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"datagen = ImageDataGenerator(\n    rescale=1.0/255,\n    validation_split=0.2,  # 80% train, 20% validation\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.3,\n    horizontal_flip=True\n)\n\n# Load training and validation data\ntrain_data = datagen.flow_from_directory(\n    data_dir,\n    target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\nval_data = datagen.flow_from_directory(\n    data_dir,\n    target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\nnum_classes = len(train_data.class_indices)\nprint(f\"Number of classes: {num_classes}\")\n\n# Get the number of classes\nnum_classes = len(train_data.class_indices)\nprint(f\"Number of classes: {num_classes}\")\n\n# Save class indices to a .pkl file\nclass_indices = train_data.class_indices\npkl_path = \"/kaggle/working/class_indices.pkl\"\nwith open(pkl_path, 'wb') as f:\n    pickle.dump(class_indices, f)\nprint(f\"class_indices saved to {pkl_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T14:18:49.928955Z","iopub.execute_input":"2025-02-09T14:18:49.929264Z","iopub.status.idle":"2025-02-09T14:18:50.364477Z","shell.execute_reply.started":"2025-02-09T14:18:49.929237Z","shell.execute_reply":"2025-02-09T14:18:50.363749Z"}},"outputs":[{"name":"stdout","text":"Found 8088 images belonging to 100 classes.\nFound 1978 images belonging to 100 classes.\nNumber of classes: 100\nNumber of classes: 100\nclass_indices saved to /kaggle/working/class_indices.pkl\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"# Load pre-trained InceptionV3 model\nbase_model = keras.applications.InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Freeze initial layers but allow fine-tuning on the top layers\nbase_model.trainable = True\nfor layer in base_model.layers[:200]:  \n    layer.trainable = False\n\n# Define the new model\nmodel = keras.Sequential([\n    base_model,\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.3),\n    layers.Dense(num_classes, activation='softmax')\n])\n\n# Compile model with Adam optimizer\noptimizer = keras.optimizers.Adam(learning_rate=0.0001)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel.summary()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T07:09:27.665644Z","iopub.execute_input":"2025-02-09T07:09:27.665978Z","iopub.status.idle":"2025-02-09T07:09:32.043574Z","shell.execute_reply.started":"2025-02-09T07:09:27.665954Z","shell.execute_reply":"2025-02-09T07:09:32.042744Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ inception_v3 (\u001b[38;5;33mFunctional\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m2048\u001b[0m)          │      \u001b[38;5;34m21,802,784\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m524,544\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m32,896\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m12,900\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ inception_v3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)          │      <span style=\"color: #00af00; text-decoration-color: #00af00\">21,802,784</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">12,900</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m22,373,124\u001b[0m (85.35 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,373,124</span> (85.35 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m15,374,628\u001b[0m (58.65 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,374,628</span> (58.65 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m6,998,496\u001b[0m (26.70 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,998,496</span> (26.70 MB)\n</pre>\n"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Set number of epochs\nepochs = 50  # Adjust as needed\n\n# Train the model\nhistory = model.fit(train_data, validation_data=val_data, epochs=epochs)\n\n# Save trained model\nmodel.save(\"classifier_inceptionv3.h5\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T07:09:39.029684Z","iopub.execute_input":"2025-02-09T07:09:39.029983Z","iopub.status.idle":"2025-02-09T10:06:33.674142Z","shell.execute_reply.started":"2025-02-09T07:09:39.029960Z","shell.execute_reply":"2025-02-09T10:06:33.673438Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 1s/step - accuracy: 0.0128 - loss: 4.6405 - val_accuracy: 0.0546 - val_loss: 4.5062\nEpoch 2/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 802ms/step - accuracy: 0.0442 - loss: 4.4625 - val_accuracy: 0.1259 - val_loss: 4.1492\nEpoch 3/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 803ms/step - accuracy: 0.1124 - loss: 4.1101 - val_accuracy: 0.2068 - val_loss: 3.8236\nEpoch 4/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 797ms/step - accuracy: 0.1689 - loss: 3.8116 - val_accuracy: 0.2513 - val_loss: 3.5202\nEpoch 5/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 799ms/step - accuracy: 0.2252 - loss: 3.5144 - val_accuracy: 0.2998 - val_loss: 3.2369\nEpoch 6/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 797ms/step - accuracy: 0.2714 - loss: 3.2845 - val_accuracy: 0.3296 - val_loss: 3.0467\nEpoch 7/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 797ms/step - accuracy: 0.3405 - loss: 2.9257 - val_accuracy: 0.3680 - val_loss: 2.8881\nEpoch 8/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 799ms/step - accuracy: 0.3682 - loss: 2.7471 - val_accuracy: 0.3913 - val_loss: 2.7830\nEpoch 9/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 792ms/step - accuracy: 0.4290 - loss: 2.4834 - val_accuracy: 0.4151 - val_loss: 2.6880\nEpoch 10/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 798ms/step - accuracy: 0.4541 - loss: 2.3647 - val_accuracy: 0.4039 - val_loss: 2.6946\nEpoch 11/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 793ms/step - accuracy: 0.4871 - loss: 2.1853 - val_accuracy: 0.4307 - val_loss: 2.6301\nEpoch 12/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 790ms/step - accuracy: 0.5179 - loss: 2.0263 - val_accuracy: 0.4540 - val_loss: 2.5536\nEpoch 13/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 788ms/step - accuracy: 0.5598 - loss: 1.8492 - val_accuracy: 0.4621 - val_loss: 2.5142\nEpoch 14/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 791ms/step - accuracy: 0.5753 - loss: 1.7731 - val_accuracy: 0.4596 - val_loss: 2.5333\nEpoch 15/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 785ms/step - accuracy: 0.5983 - loss: 1.6404 - val_accuracy: 0.4717 - val_loss: 2.5079\nEpoch 16/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 795ms/step - accuracy: 0.6346 - loss: 1.5037 - val_accuracy: 0.4702 - val_loss: 2.5088\nEpoch 17/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 785ms/step - accuracy: 0.6795 - loss: 1.3197 - val_accuracy: 0.4788 - val_loss: 2.5833\nEpoch 18/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 783ms/step - accuracy: 0.6839 - loss: 1.2868 - val_accuracy: 0.4793 - val_loss: 2.5588\nEpoch 19/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 793ms/step - accuracy: 0.6952 - loss: 1.2365 - val_accuracy: 0.4727 - val_loss: 2.6583\nEpoch 20/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 805ms/step - accuracy: 0.7230 - loss: 1.1193 - val_accuracy: 0.4869 - val_loss: 2.6437\nEpoch 21/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 803ms/step - accuracy: 0.7267 - loss: 1.0721 - val_accuracy: 0.4914 - val_loss: 2.6697\nEpoch 22/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 800ms/step - accuracy: 0.7562 - loss: 0.9808 - val_accuracy: 0.4924 - val_loss: 2.6432\nEpoch 23/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 802ms/step - accuracy: 0.7605 - loss: 0.9232 - val_accuracy: 0.4975 - val_loss: 2.6337\nEpoch 24/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 807ms/step - accuracy: 0.7837 - loss: 0.8446 - val_accuracy: 0.4949 - val_loss: 2.6939\nEpoch 25/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 805ms/step - accuracy: 0.7977 - loss: 0.8180 - val_accuracy: 0.4975 - val_loss: 2.7647\nEpoch 26/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 819ms/step - accuracy: 0.8084 - loss: 0.7389 - val_accuracy: 0.4960 - val_loss: 2.7235\nEpoch 27/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 807ms/step - accuracy: 0.8154 - loss: 0.7157 - val_accuracy: 0.4949 - val_loss: 2.7557\nEpoch 28/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 824ms/step - accuracy: 0.8258 - loss: 0.6899 - val_accuracy: 0.5071 - val_loss: 2.6979\nEpoch 29/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 819ms/step - accuracy: 0.8353 - loss: 0.6398 - val_accuracy: 0.4975 - val_loss: 2.8822\nEpoch 30/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 806ms/step - accuracy: 0.8388 - loss: 0.6376 - val_accuracy: 0.5005 - val_loss: 2.8318\nEpoch 31/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 812ms/step - accuracy: 0.8538 - loss: 0.5812 - val_accuracy: 0.5081 - val_loss: 2.9161\nEpoch 32/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 803ms/step - accuracy: 0.8583 - loss: 0.5669 - val_accuracy: 0.4965 - val_loss: 2.8923\nEpoch 33/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 808ms/step - accuracy: 0.8617 - loss: 0.5503 - val_accuracy: 0.5101 - val_loss: 2.9764\nEpoch 34/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 809ms/step - accuracy: 0.8625 - loss: 0.5327 - val_accuracy: 0.5066 - val_loss: 3.0184\nEpoch 35/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 812ms/step - accuracy: 0.8636 - loss: 0.5238 - val_accuracy: 0.5071 - val_loss: 3.0441\nEpoch 36/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 812ms/step - accuracy: 0.8825 - loss: 0.4633 - val_accuracy: 0.5030 - val_loss: 3.0863\nEpoch 37/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 813ms/step - accuracy: 0.8892 - loss: 0.4347 - val_accuracy: 0.5091 - val_loss: 3.1581\nEpoch 38/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 809ms/step - accuracy: 0.8823 - loss: 0.4558 - val_accuracy: 0.5071 - val_loss: 2.9690\nEpoch 39/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 817ms/step - accuracy: 0.8898 - loss: 0.4425 - val_accuracy: 0.5035 - val_loss: 3.0415\nEpoch 40/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 819ms/step - accuracy: 0.8874 - loss: 0.4302 - val_accuracy: 0.4975 - val_loss: 3.1759\nEpoch 41/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 818ms/step - accuracy: 0.9002 - loss: 0.3926 - val_accuracy: 0.5081 - val_loss: 3.1942\nEpoch 42/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 812ms/step - accuracy: 0.8914 - loss: 0.4291 - val_accuracy: 0.5076 - val_loss: 3.3117\nEpoch 43/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 812ms/step - accuracy: 0.8999 - loss: 0.3706 - val_accuracy: 0.5025 - val_loss: 3.2062\nEpoch 44/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 808ms/step - accuracy: 0.9043 - loss: 0.3929 - val_accuracy: 0.5147 - val_loss: 3.1926\nEpoch 45/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 816ms/step - accuracy: 0.9108 - loss: 0.3538 - val_accuracy: 0.4975 - val_loss: 3.2015\nEpoch 46/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 810ms/step - accuracy: 0.9163 - loss: 0.3394 - val_accuracy: 0.5076 - val_loss: 3.2367\nEpoch 47/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 815ms/step - accuracy: 0.9155 - loss: 0.3476 - val_accuracy: 0.5137 - val_loss: 3.2019\nEpoch 48/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 814ms/step - accuracy: 0.9215 - loss: 0.3197 - val_accuracy: 0.5076 - val_loss: 3.1723\nEpoch 49/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 816ms/step - accuracy: 0.9188 - loss: 0.3053 - val_accuracy: 0.5051 - val_loss: 3.2409\nEpoch 50/50\n\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 810ms/step - accuracy: 0.9216 - loss: 0.3140 - val_accuracy: 0.5096 - val_loss: 3.2612\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Load the saved Inception model\nmodel = tf.keras.models.load_model(\"/kaggle/working/classifier_inceptionv3.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:45:26.856615Z","iopub.execute_input":"2025-02-09T13:45:26.856979Z","iopub.status.idle":"2025-02-09T13:45:28.774265Z","shell.execute_reply.started":"2025-02-09T13:45:26.856950Z","shell.execute_reply":"2025-02-09T13:45:28.773591Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def load_and_preprocess_image(image_path, target_size=(224, 224)):\n    # Load image\n    img = image.load_img(image_path, target_size=target_size)\n    \n    # Convert to array\n    img_array = image.img_to_array(img)\n    \n    # Expand dimensions to match the input shape of the model (1, 224, 224, 3)\n    img_array = np.expand_dims(img_array, axis=0)\n    \n    # Preprocess the image (normalization)\n    img_array = preprocess_input(img_array)\n    \n    return img_array\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:45:42.811468Z","iopub.execute_input":"2025-02-09T13:45:42.811769Z","iopub.status.idle":"2025-02-09T13:45:42.816265Z","shell.execute_reply.started":"2025-02-09T13:45:42.811747Z","shell.execute_reply":"2025-02-09T13:45:42.815315Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def predict_plant(image_path, model, class_indices, df):\n    # Preprocess the input image\n    img_array = load_and_preprocess_image(image_path)\n    \n    # Make predictions\n    predictions = model.predict(img_array)\n    \n    # Get the predicted class index\n    predicted_class_idx = np.argmax(predictions, axis=1)[0]\n    \n    # Get the predicted plant name based on class index\n    predicted_plant_name = list(class_indices.keys())[list(class_indices.values()).index(predicted_class_idx)]\n    \n    # Retrieve plant details from the dataset (df)\n    plant_details = df[df['Scientific_name'] == predicted_plant_name].to_dict(orient='records')[0]\n    \n    return predicted_plant_name, plant_details\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:45:52.351865Z","iopub.execute_input":"2025-02-09T13:45:52.352261Z","iopub.status.idle":"2025-02-09T13:45:52.359057Z","shell.execute_reply.started":"2025-02-09T13:45:52.352228Z","shell.execute_reply":"2025-02-09T13:45:52.357854Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Load plant dataset containing scientific names and details (df)\ndf = pd.read_excel(\"/kaggle/input/sci-data/sci.xlsx\")  # Update with your dataset file path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:46:19.247075Z","iopub.execute_input":"2025-02-09T13:46:19.247380Z","iopub.status.idle":"2025-02-09T13:46:19.300082Z","shell.execute_reply.started":"2025-02-09T13:46:19.247338Z","shell.execute_reply":"2025-02-09T13:46:19.299049Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport os\n\n# Load the saved Inception model\nmodel = tf.keras.models.load_model(\"/kaggle/working/classifier_inceptionv3.h5\")\n\n# Load the plant dataset\ndf = pd.read_excel(\"/kaggle/input/sci-data/sci.xlsx\")  # Update with your dataset file path\n\n# Define dataset path and parameters\ndata_dir = \"/kaggle/input/med-images/images\"  # Update with your image dataset path\nimg_size = (224, 224)\nbatch_size = 32\n\n# Create the ImageDataGenerator for loading the images\ndatagen = ImageDataGenerator(\n    rescale=1.0/255,\n    validation_split=0.2  # Assuming you split the data earlier\n)\n\n# Load training data to get class_indices\ntrain_data = datagen.flow_from_directory(\n    data_dir,\n    target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'  # Load the training subset\n)\n\n# Retrieve class_indices\nclass_indices = train_data.class_indices\n\n# Function to preprocess the image\ndef load_and_preprocess_image(image_path, target_size=(224, 224)):\n    img = image.load_img(image_path, target_size=target_size)  # Ensure the 'image' module is imported\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)\n    img_array = preprocess_input(img_array)\n    return img_array\n\n# Predict function\ndef predict_plant(image_path, model, class_indices, df):\n    img_array = load_and_preprocess_image(image_path)\n    predictions = model.predict(img_array)\n    predicted_class_idx = np.argmax(predictions, axis=1)[0]\n    predicted_plant_name = list(class_indices.keys())[list(class_indices.values()).index(predicted_class_idx)]\n    plant_details = df[df['Scientific_name'] == predicted_plant_name].to_dict(orient='records')[0]\n    return predicted_plant_name, plant_details\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:54:27.235602Z","iopub.execute_input":"2025-02-09T13:54:27.235900Z","iopub.status.idle":"2025-02-09T13:54:32.695488Z","shell.execute_reply.started":"2025-02-09T13:54:27.235877Z","shell.execute_reply":"2025-02-09T13:54:32.694818Z"}},"outputs":[{"name":"stdout","text":"Found 8088 images belonging to 100 classes.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# Input image path\nimage_path = \"/kaggle/input/med-images/images/Celastrus paniculatus/Image_30.jpg\"  # Update with your test image path\n\n# Predict\nplant_name, plant_details = predict_plant(image_path, model, class_indices, df)\n\n# Output\nprint(f\"Predicted Plant: {plant_name}\")\nprint(f\"Plant Details: {plant_details}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T14:01:21.571411Z","iopub.execute_input":"2025-02-09T14:01:21.571726Z","iopub.status.idle":"2025-02-09T14:01:21.659008Z","shell.execute_reply.started":"2025-02-09T14:01:21.571704Z","shell.execute_reply":"2025-02-09T14:01:21.658101Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\nPredicted Plant: Celastrus paniculatus\nPlant Details: {'Scientific_name': 'Celastrus paniculatus', 'Telugu_name': 'Pengu', 'Parts_used': 'Leaves, Seeds, Oil, Bark', 'Uses': 'Nervine Tonic', 'Grown_Area': 'Natural Source in Forest Area of above\\n500 mtr. Height', 'Preparation_method': \"The seeds are diluted in cow's water and applied to sores on the gums. Powder the seeds and give 1 spoonful 2-3 times a day to relieve cough, constipation and flatulence.\", 'Preparation_method(Telugu)': 'వల్లరై, తులసి ఆకులు, మిరియాలపొడిని తీసుకుని మిరప పొడిలా గ్రైండ్ చేసుకోవాలి. అన్ని సూరాలు చుట్టి రెండుసార్లు ఇస్తాయి. ఆకులను గ్రైండ్ చేయడం వల్ల ఏనుగు పాదాల వాపు, దద్దుర్లు, కణితుల నుంచి ఉపశమనం లభిస్తుంది. మెంతి ఆకులతో కషాయం తయారుచేసి పిల్లలకు 1 నుంచి 2 కోన్లు ఇస్తే గుండెల్లో మంట, కడుపు సమస్యలు నయం అవుతాయి. ఆవు పాలలో ఆకు పొడిని కలపడం వల్ల జ్ఞాపకశక్తి పెరుగుతుంది. రోజూ నాలుగు ఆకులు తింటే జ్ఞానం పెరుగుతుంది. వల్లరై రుతుస్రావాన్ని తీవ్రతరం చేసే (అబార్షన్) ప్రభావాన్ని కలిగి ఉంటుంది. ఆడ పిల్లలకు ఆకు ఎక్కువగా ఇవ్వకూడదు. గర్భిణీ స్త్రీలు మరియు మూర్ఛతో బాధపడుతున్నవారు వల్లరైని ఔషధంగా లేదా ఆహారంగా తీసుకోకూడదు.'}\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"import requests\nfrom PIL import Image\nfrom io import BytesIO\nimport numpy as np\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\n\n# Function to download and preprocess image from URL\ndef download_image_from_url(url, target_size=(224, 224)):\n    response = requests.get(url)  # Download the image\n    img = Image.open(BytesIO(response.content))  # Open the image\n    img = img.resize(target_size)  # Resize to match model input\n    img_array = image.img_to_array(img)  # Convert to array\n    img_array = np.expand_dims(img_array, axis=0)  # Expand dimensions to match model input\n    img_array = preprocess_input(img_array)  # Preprocess the image\n    return img_array\n\n# Predict function for URL-based image\ndef predict_plant_from_url(image_url, model, class_indices, df):\n    img_array = download_image_from_url(image_url)  # Download and preprocess image from URL\n    predictions = model.predict(img_array)  # Predict the class\n    predicted_class_idx = np.argmax(predictions, axis=1)[0]\n    predicted_plant_name = list(class_indices.keys())[list(class_indices.values()).index(predicted_class_idx)]\n    plant_details = df[df['Scientific_name'] == predicted_plant_name].to_dict(orient='records')[0]\n    return predicted_plant_name, plant_details\n\n# Example: Predict from a URL\nimage_url = \"https://bloomingfloret.in/cdn/shop/files/6091-2_1.jpg?v=1725259539\"  # Replace with the actual URL of the image\nplant_name_url, plant_details_url = predict_plant_from_url(image_url, model, class_indices, df)\n\n# Output\nprint(f\"Predicted Plant (URL): {plant_name_url}\")\nprint(f\"Plant Details (URL): {plant_details_url}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T14:40:51.274957Z","iopub.execute_input":"2025-02-09T14:40:51.275314Z","iopub.status.idle":"2025-02-09T14:40:51.466668Z","shell.execute_reply.started":"2025-02-09T14:40:51.275285Z","shell.execute_reply":"2025-02-09T14:40:51.465880Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\nPredicted Plant (URL): Aloe vera\nPlant Details (URL): {'Scientific_name': 'Aloe vera', 'Telugu_name': 'Kalabanda', 'Parts_used': 'Leaf, Root', 'Uses': ' Tonic, Emmenagogue, Refrigerant', 'Grown_Area': 'All over Andhra Pradhesh', 'Preparation_method': 'Kumari (a) Rice cactus: Take the blood and rind after cutting the young lobe with the skin and adding cumin seeds.\\nMixed discharge and body heat are cured. Wash the rice of the aloe several times, add some Chinese sugar to it and let the water drain in it to hang it in small pieces. If this is done in the eyes, conjunctivitis etc. will become. Its leaves can be used to cure chronic ulcers. Take its rice, oil it and rub it on the head to induce hair growth all over the head. Applying its milk on the eyes cures sores in the eyeballs. Take its rice and apply it on your face to reduce acne. Take 3 teaspoons of its rice daily and grind it with the required amount of native sugar and make the juice in the morning on an empty stomach. Applying aloe vera juice on the body will relieve skin dryness\\nNote: Wash the aloe vera leaf seven times before use.', 'Preparation_method(Telugu)': 'కుమారి (ఎ) కలబంద: జీలకర్ర, పంచదార మిఠాయితో పొట్టు తీయి రక్తం, మజ్జిగ తినాలి. మిశ్రమ మలవిసర్జన, శరీర వేడి నయమవుతాయి. కలబంద బియ్యాన్ని పలుమార్లు శుభ్రంగా కడిగి, అందులో కొంత చైనా మిఠాయి వేసి, చిన్న ముక్కగా చుట్టి వేలాడదీస్తే అందులో నీరు కారుతుంది. ఇది కళ్ళలో కంటే ఎక్కువగా కనిపిస్తుంది. దీర్ఘకాలిక అల్సర్లను నయం చేయడానికి దీని లోబ్ ఉపయోగపడుతుంది. దాని బియ్యాన్ని నూనెలో మరిగించి తలకు రుద్దితే తలపై వెంట్రుకలు పెరిగి నిద్ర పడుతుంది. దీని పాలను కళ్లకు రాసుకుంటే కనురెప్పలోని పుండ్లు నయమవుతాయి. దాని బియ్యాన్ని తీసుకుని ముఖానికి అప్లై చేస్తే మొటిమలు తగ్గుతాయి. రోజూ 3 టీస్పూన్ల అన్నం తీసుకుని దానికి సరిపడా పంచదార కలిపి ఉదయాన్నే పరగడుపున జ్యూస్ లా గ్రైండ్ చేస్తే బాడీ హీట్ తగ్గుతుంది. కలబంద అన్నాన్ని శరీరానికి అప్లై చేయడం వల్ల చర్మం పొడిబారడం తొలగిపోతుంది. గమనిక: అలోవెరా ఫ్లాప్ ను ఉపయోగించే ముందు ఏడుసార్లు కడగాలి.'}\n","output_type":"stream"}],"execution_count":41}]}